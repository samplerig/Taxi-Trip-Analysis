{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "02a2deea-1a7b-4338-9be2-efb73b8ff097",
    "_execution_state": "idle",
    "_uuid": "4a7a4d22c10d6672afd30df6ddc745187dd26a29",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hi everyone, I want to approach this challenge using Deep Neural Networks of Tensorflow. For the current settings and training steps = 1000 steps, my RMSLE score is 0.44. I believe a much better result can be achieved with more investigating into feature expanding and engineering and especially more training steps (e.g. more computing power). If you have any input or want to join me as a team in this approach, feel free to contact me.\n",
    "\n",
    "I'm new to Kaggle so I'm not sure how to run code on online kernels. I have several steps of data processing and model training in separated code files and save the new datasets as csv files for later use. So let I will post the codes separately with detail instructions. \n",
    "\n",
    "Special thanks to @Mathijs Waegemakers for sharing the Weather Dataset and the code.\n",
    "\n",
    "*** Traveling Distance:\n",
    "\n",
    "With the given data fields “pickup_longtitude”, “pickup_latitude”, “dropoff_longtitude” and “dropoff_latitude”, I can try to calculate the traveling distance of each taxi trip. Even though a highly accurate distance can be obtained via Google Maps API, the limit of 2,500 API calls per day of Google has made that approach inefficient for execution. Therefore, I decided to utilize the Python package “geopy” to estimate distance between 2 points in meter measurement. When you run the \"distance_geopy.py\" code as below, it will create 2 csv files: \"meter_train.csv\" and \"meter_test.csv\" accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "3b6cd537-d8ae-4f3f-bdf7-cdeae937a1d3",
    "_uuid": "372e21a249db92ae45b466650fca42cd582846f2"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "train.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-63b602666217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgreat_circle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mN1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows)\u001b[0m\n\u001b[1;32m   1510\u001b[0m                 \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rbU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m                 \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_file_openers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: train.csv not found."
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Glenn-Galang\n",
    "'''\n",
    "from numpy import genfromtxt\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "train = genfromtxt('train.csv', delimiter=',')\n",
    "\n",
    "N1 = train.shape[0]\n",
    "\n",
    "for i in range(1,N1):\n",
    "\tpick_up = (train[i,6],train[i,5])\n",
    "\tdrop_off = (train[i,8],train[i,7])\n",
    "\tdistance = 1000*(great_circle(pick_up,drop_off).km)\n",
    "\twith open(\"meter_train.csv\", \"a\") as myfile:\n",
    "\t\t\tmyfile.write(str(distance)+\",\\n\")\n",
    "\t\t\n",
    "test = genfromtxt('test.csv', delimiter=',')\n",
    "\n",
    "N2 = test.shape[0]\n",
    "\n",
    "for i in range(1,N2):\n",
    "\tpick_up = (test[i,5],test[i,4])\n",
    "\tdrop_off = (test[i,7],test[i,6])\n",
    "\tdistance = 1000*(great_circle(pick_up,drop_off).km)\n",
    "\twith open(\"meter_test.csv\", \"a\") as myfile:\n",
    "\t\tmyfile.write(str(distance)+\",\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9ccd59a1-6769-4253-bf47-dd071d647a6f",
    "_execution_state": "idle",
    "_uuid": "4c11fea345611aa2a2397021cdf18233aed75f02",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "***  Distance from city center:\n",
    "\n",
    "In general, there will always be more traffic in the city center than other suburban neighbourhoods. So I again using Python package \"geopy\" to calculate the radical distance between city center (set as Central Park) to pickup and dropoff points. When you run the \"distance_from_central.py\" code as below, it will create 2 csv files: \"distance_from_central_train.csv\" and \"distance_from_central_test.csv\" accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "986fcbc8-37d8-4040-a186-7ff2cca2bb48",
    "_execution_state": "idle",
    "_uuid": "332a06afd055a73fe6f90543dc9ead5f106979b7",
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "train.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-414aa92792a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcentral\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m40.781277\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m73.966622\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mN1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows)\u001b[0m\n\u001b[1;32m   1510\u001b[0m                 \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rbU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m                 \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_file_openers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: train.csv not found."
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Glenn-Galang\n",
    "'''\n",
    "from numpy import genfromtxt\n",
    "from geopy.distance import great_circle\n",
    "    \n",
    "central = (40.781277,-73.966622)\n",
    "\t\n",
    "train = genfromtxt('train.csv', delimiter=',')\n",
    "\n",
    "N1 = train.shape[0]\n",
    "\n",
    "for i in range(1,N1):\n",
    "\tpick_up = (train[i,6],train[i,5])\n",
    "\tdrop_off = (train[i,8],train[i,7])\n",
    "\tdistance_pickup = 1000*(great_circle(central,pick_up).km)\n",
    "\tdistance_dropoff = 1000*(great_circle(central,drop_off).km)\n",
    "\twith open(\"distance_from_central_train.csv\", \"a\") as myfile:\n",
    "\t\t\tmyfile.write(str(distance_pickup)+\",\"+str(distance_dropoff)+\",\\n\")\n",
    "\t\t\n",
    "test = genfromtxt('test.csv', delimiter=',')\n",
    "\n",
    "N2 = test.shape[0]\n",
    "\n",
    "for i in range(1,N2):\n",
    "\tpick_up = (test[i,5],test[i,4])\n",
    "\tdrop_off = (test[i,7],test[i,6])\n",
    "\tdistance_pickup = 1000*(great_circle(central,pick_up).km)\n",
    "\tdistance_dropoff = 1000*(great_circle(central,drop_off).km)\n",
    "\twith open(\"distance_from_central_test.csv\", \"a\") as myfile:\n",
    "\t\t\tmyfile.write(str(distance_pickup)+\",\"+str(distance_dropoff)+\",\\n\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6be492d3-6ef1-45e6-8035-e5d8bcdb3a26",
    "_execution_state": "idle",
    "_uuid": "d09dcdfc32b76f1bffb76ff34318a2a2db8317e9",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "*** Data Processing:\n",
    "\n",
    "Now I have several datasets in separated CSV files. I will run \"data_processing.py\" to combine these datasets, conduct some exploratory data analysis and data cleaning.  I preprossessed the Weather dataset to change all the \"T\" (which mean \"Trace\") in 3 columns \"Precipitation\", \"Snow_fall\" and \"Snow_depth\" to 0.005. The detailed steps are in the comment of each code parts. The final datasets which will be used for model training and testing will be saved as \"train_df.csv\", \"val_df.csv\" and \"test_df.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "4414e03c-daaa-4c4a-8be7-a38a5be622d9",
    "_execution_state": "idle",
    "_uuid": "bdc54421d2af823384c7cea2e9f433ba158b9bb1",
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d34a312cc56a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# import original train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# import original train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Glenn-Galang\n",
    "'''\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import original train dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# import original train dataset\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# import weather dataset\n",
    "weat_data = pd.read_csv('weather_data_nyc_centralpark_2016.csv')\n",
    "\n",
    "# function to add snow weather by pickup date\n",
    "weat_data['date'] = pd.to_datetime( weat_data['date'] ).dt.date\n",
    "weat_data.set_index('date', inplace = True)\n",
    "def addWeather( df ):\n",
    "    \n",
    "    df['date'] =  pd.to_datetime(df['pickup_datetime']).dt.date\n",
    "    \n",
    "    dates = df['date'].unique()\n",
    "    \n",
    "    df.set_index('date', inplace = True)\n",
    "   \n",
    "    weat_cols = ['precipitation', 'snow_fall', 'snow_depth']\n",
    "    \n",
    "    for col in weat_cols:\n",
    "        df[col] = np.nan\n",
    "        \n",
    "        for date in dates:\n",
    "            val = weat_data.loc[date, col]\n",
    "            \n",
    "            if( 'T' != val ):\n",
    "                df.loc[date, col] = float(val)\n",
    "        \n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "#1. Processing train dataset\n",
    "\n",
    "# import weather data\n",
    "train_df = addWeather(train_df)\n",
    "\n",
    "# import distance data, no header, then attach it to train_df\n",
    "meter_train_df = pd.read_csv('meter_train.csv', header=None)\n",
    "train_df['meter'] = meter_train_df[0]\n",
    "\n",
    "# import distance to central data, no header, then attach it to train_df\n",
    "distance_from_central_train_df = pd.read_csv('distance_from_central_train.csv', header=None)\n",
    "train_df['pickup_distance'] = distance_from_central_train_df[0]\n",
    "train_df['dropoff_distance'] = distance_from_central_train_df[1]\n",
    "\n",
    "# compute pickup hour, date, month for each ride\n",
    "train_df['pickup_datetime'] = pd.to_datetime(train_df['pickup_datetime'])\n",
    "train_df['pickup_hour'] = train_df.pickup_datetime.dt.hour\n",
    "train_df['pickup_date'] = train_df.pickup_datetime.dt.day\n",
    "train_df['pickup_month'] = train_df.pickup_datetime.dt.month\n",
    "\n",
    "# comput pickup day of the week for each ride from 0 (Monday) to 6 (Sunday) \n",
    "train_df['day_week'] = train_df.pickup_datetime.dt.weekday\n",
    "\n",
    "# remove columns that we don't need\n",
    "del train_df['id']\n",
    "del train_df['pickup_datetime']\n",
    "del train_df['dropoff_datetime']\n",
    "del train_df['pickup_longitude']\n",
    "del train_df['pickup_latitude']\n",
    "del train_df['dropoff_longitude']\n",
    "del train_df['dropoff_latitude']\n",
    "\n",
    "# get the list of column names\n",
    "list(train_df)\n",
    "\n",
    "# checking our target Y: trip_duration and plot\n",
    "plt.scatter(range(train_df.shape[0]), np.sort(train_df.trip_duration.values))\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('trip duration')\n",
    "plt.show()\n",
    "\n",
    "# remove some unsual long or short trips and plot again\n",
    "q1 = train_df.trip_duration.quantile(0.001)\n",
    "q2 = train_df.trip_duration.quantile(0.999)\n",
    "train_df = train_df[(train_df.trip_duration > q1) & (train_df.trip_duration < q2)]\n",
    "plt.scatter(range(train_df.shape[0]), np.sort(train_df.trip_duration.values))\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('trip duration')\n",
    "plt.show()\n",
    "\n",
    "# checking our meter data field and plot\n",
    "plt.scatter(range(train_df.shape[0]), np.sort(train_df.meter.values))\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('meter')\n",
    "plt.show()\n",
    "\n",
    "# remove some trip shorter than 100 meters and longer than 80 km and plot again\n",
    "train_df = train_df[(train_df.meter > 100) & (train_df.meter < 80000)]\n",
    "plt.scatter(range(train_df.shape[0]), np.sort(train_df.meter.values))\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('meter')\n",
    "plt.show()\n",
    "\n",
    "# count values in other X columns to detect unusual values\n",
    "train_df['vendor_id'].value_counts()\n",
    "train_df['passenger_count'].value_counts()\n",
    "train_df['store_and_fwd_flag'].value_counts()\n",
    "train_df['pickup_hour'].value_counts()\n",
    "train_df['pickup_date'].value_counts()\n",
    "train_df['pickup_month'].value_counts()\n",
    "train_df['day_week'].value_counts()\n",
    "\n",
    "# remove trips with 0, 8 or 9 passenger(s) and check again\n",
    "train_df = train_df[(train_df.passenger_count < 8) & (train_df.passenger_count != 0)]\n",
    "train_df['passenger_count'].value_counts()\n",
    "\n",
    "# (optional) move trip_duration to end column\n",
    "train_df['duration'] = train_df['trip_duration']\n",
    "del train_df['trip_duration']\n",
    "\n",
    "# split train and valuation\n",
    "train=train_df.sample(frac=0.8,random_state=200)\n",
    "val=train_df.drop(train.index)\n",
    "\n",
    "# write the processed train and valuation dataset to csv\n",
    "train.to_csv('train_df.csv', index=False)\n",
    "val.to_csv('val_df.csv', index=False)\n",
    "\n",
    "#2. Processing test dataset\n",
    "\n",
    "# import weather data\n",
    "test_df = addWeather(test_df)\n",
    "\n",
    "# import distance data, no header, then attach it to train_df\n",
    "meter_test_df = pd.read_csv('meter_test.csv', header=None)\n",
    "test_df['meter'] = meter_test_df[0]\n",
    "\n",
    "# import distance to central data, no header, then attach it to train_df\n",
    "distance_from_central_test_df = pd.read_csv('distance_from_central_test.csv', header=None)\n",
    "test_df['pickup_distance'] = distance_from_central_test_df[0]\n",
    "test_df['dropoff_distance'] = distance_from_central_test_df[1]\n",
    "\n",
    "# get the list of column names\n",
    "list(test_df)\n",
    "\n",
    "# compute pickup hour, date, month for each ride\n",
    "test_df['pickup_datetime'] = pd.to_datetime(test_df['pickup_datetime'])\n",
    "test_df['pickup_hour'] = test_df.pickup_datetime.dt.hour\n",
    "test_df['pickup_date'] = test_df.pickup_datetime.dt.day\n",
    "test_df['pickup_month'] = test_df.pickup_datetime.dt.month\n",
    "\n",
    "# comput pickup day of the week for each ride from 0 (Monday) to 6 (Sunday) \n",
    "test_df['day_week'] = test_df.pickup_datetime.dt.weekday\n",
    "\n",
    "# remove columns that we don't need\n",
    "del test_df['id']\n",
    "del test_df['pickup_datetime']\n",
    "del test_df['pickup_longitude']\n",
    "del test_df['pickup_latitude']\n",
    "del test_df['dropoff_longitude']\n",
    "del test_df['dropoff_latitude']\n",
    "\n",
    "# write the processed test_df to csv\n",
    "test_df.to_csv('test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e151dc4-37b8-4a97-a86b-671adfa2f217",
    "_execution_state": "idle",
    "_uuid": "a21f7e0279d8dda5c2e78304e6bf7f4535ce92b5",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "*** Model training, evaluating and prediction:\n",
    "\n",
    "I use a straight-forward Deep Neural Networks Regression model from Tensorflow with 3 hidden layers and hidden units = [10,10,10]. With current 1000 training steps, my RMSLE score is 0.44. I also included some other metrics from package \"sklearn\" for reference purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b528cbc3-5689-4f9c-8779-49c6304a8b3d",
    "_execution_state": "idle",
    "_uuid": "e4e74249e96a8b58b078ceb8aa796485be780d59",
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'train_df.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0be194b4c996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mevaluate_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'train_df.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn import metrics\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "# (Optional) Extra logging \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Import data\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "evaluate_df = pd.read_csv('val_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "\n",
    "# define Root Mean Squared Logarithmic Error for evaluation\n",
    "def rmsle(real,predicted):\n",
    "    sum=0.000\n",
    "    length=len(predicted)\n",
    "    for x in range(length):\n",
    "        p = np.log(predicted[x]+1)\n",
    "        r = np.log(real[x]+1)\n",
    "        sum = sum + (p - r)**2\n",
    "    return (sum/length)**0.5\n",
    "        \n",
    "\n",
    "MODEL_DIR = \"tf_model_full\"\n",
    "\n",
    "categorical_features = ['vendor_id', 'passenger_count', 'store_and_fwd_flag', 'pickup_hour', 'pickup_date', 'pickup_month', 'day_week']\n",
    "continuous_features = ['meter', 'pickup_distance', 'dropoff_distance', 'precipitation', 'snow_fall', 'snow_depth']\n",
    "LABEL_COLUMN = 'duration'\n",
    "\n",
    "# convert types of categorical features to string\n",
    "for k in categorical_features:\n",
    "    train_df[k] = train_df[k].apply(str)\n",
    "    evaluate_df[k] = evaluate_df[k].apply(str)\n",
    "    test_df[k] = test_df[k].apply(str)\n",
    "\n",
    "# Converting Data into Tensors\n",
    "def input_fn(df, training = True):\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.constant(df[k].values)\n",
    "                       for k in continuous_features}\n",
    "\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "        indices=[[i, 0] for i in range(df[k].size)],\n",
    "        values=df[k].values,\n",
    "        dense_shape=[df[k].size, 1])\n",
    "        for k in categorical_features}\n",
    "\n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(list(continuous_cols.items()) +\n",
    "                        list(categorical_cols.items()))\n",
    "\n",
    "    if training:\n",
    "        # Converts the label column into a constant Tensor.\n",
    "        label = tf.constant(df[LABEL_COLUMN].values)\n",
    "\n",
    "        # Returns the feature columns and the label.\n",
    "        return feature_cols, label\n",
    "    \n",
    "    # Returns the feature columns    \n",
    "    return feature_cols\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(train_df)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_fn(evaluate_df)\n",
    "\n",
    "def test_input_fn():\n",
    "    return input_fn(test_df, False)\n",
    "    \n",
    "# engineering features\n",
    "engineered_features = []\n",
    "\n",
    "for continuous_feature in continuous_features:\n",
    "    engineered_features.append(\n",
    "        tf.contrib.layers.real_valued_column(continuous_feature))\n",
    "\n",
    "\n",
    "for categorical_feature in categorical_features:\n",
    "    sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "        categorical_feature, hash_bucket_size=1000)\n",
    "\n",
    "    engineered_features.append(tf.contrib.layers.embedding_column(sparse_id_column=sparse_column, dimension=16,\n",
    "                                                                  combiner=\"sum\"))\n",
    "# defining model\n",
    "regressor = tf.contrib.learn.DNNRegressor(\n",
    "    feature_columns=engineered_features, hidden_units=[10,10,10], model_dir=MODEL_DIR)\n",
    "\n",
    "# Fit the model\n",
    "wrap = regressor.fit(input_fn=train_input_fn, steps=1000)\n",
    "    \n",
    "# Evaluate model with rmsle metric\n",
    "val_df = regressor.predict_scores(input_fn=eval_input_fn)\n",
    "val_prediction = list(itertools.islice(val_df,evaluate_df['duration'].size))\n",
    "val_prediction_array = np.asfarray(val_prediction)\n",
    "val_y_array = np.asfarray(evaluate_df['duration']) \n",
    "print(rmsle(val_y_array, val_prediction_array))\n",
    "'''\n",
    "# Evaluating Our Model    \n",
    "print('Evaluating ...')\n",
    "results = regressor.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n",
    "\n",
    "# Other evaluation metrics for reference\n",
    "print(metrics.explained_variance_score(val_y_array, val_prediction_array))\n",
    "print(metrics.mean_absolute_error(val_y_array, val_prediction_array))\n",
    "print(metrics.mean_squared_error(val_y_array, val_prediction_array))\n",
    "print(metrics.median_absolute_error(val_y_array, val_prediction_array))\n",
    "print(metrics.r2_score(val_y_array, val_prediction_array))\n",
    "    \n",
    "# Predict with Our Model\n",
    "predicted_output = regressor.predict_scores(input_fn=test_input_fn)\n",
    "predictions = list(itertools.islice(predicted_output,test_df['vendor_id'].size))\n",
    "prediction_array = np.asfarray(predictions)\n",
    "\n",
    "# write predictions to csv\n",
    "np.savetxt(\"prediction.csv\", prediction_array, delimiter=\",\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3a93dfa1-4ed4-4faa-bcb1-8c3be63fe9f8",
    "_execution_state": "idle",
    "_uuid": "dda39b117fe5232acfcf43a5d5c758f298ac3d46",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "This still has a lot of room for improvement. So if you have any idea or would like to join my team to work on Deep Neural Networks, feel free to let me know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
